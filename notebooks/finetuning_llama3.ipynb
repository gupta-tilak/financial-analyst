{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers==4.41.0\n",
    "!pip install peft==0.11.0\n",
    "!pip install datasets==2.19.0\n",
    "!pip install trl==0.8.6\n",
    "!pip install bitsandbytes==0.43.1\n",
    "!pip install accelerate==0.30.1\n",
    "!pip install torch==2.3.0\n",
    "!pip install neptune-client  # For experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs-legacy.neptune.ai/setup/upgrading/\n",
      "/Users/guptatilak/Documents/projects/ml/financial-analyst/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch detected 0 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import neptune\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Set GPU configuration\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Adjust based on available GPUs\n",
    "print(f\"PyTorch detected {torch.cuda.device_count()} GPU(s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Finance Alpaca: 68912 samples\n",
      "Formatted dataset size: 68912\n"
     ]
    }
   ],
   "source": [
    "def load_financial_datasets():\n",
    "    \"\"\"Load and combine financial datasets\"\"\"\n",
    "    \n",
    "    # Load Finance Alpaca dataset\n",
    "    try:\n",
    "        finance_alpaca = load_dataset(\"gbharti/finance-alpaca\", split=\"train\")\n",
    "        print(f\"Loaded Finance Alpaca: {len(finance_alpaca)} samples\")\n",
    "    except:\n",
    "        print(\"Finance Alpaca not available, creating sample dataset\")\n",
    "        # Create sample financial data if dataset not accessible\n",
    "        sample_data = [\n",
    "            {\n",
    "                \"instruction\": \"What is the P/E ratio and how is it calculated?\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": \"The P/E ratio (Price-to-Earnings ratio) is calculated by dividing the market price per share by the earnings per share (EPS). It indicates how much investors are willing to pay for each dollar of earnings.\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Explain the importance of SEC 10-K filings for investors.\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": \"SEC 10-K filings are annual reports that provide a comprehensive overview of a company's business, financial condition, and results of operations. They are crucial for investors as they contain audited financial statements, risk factors, and management discussion.\"\n",
    "            }\n",
    "        ]\n",
    "        finance_alpaca = Dataset.from_list(sample_data)\n",
    "    \n",
    "    return finance_alpaca\n",
    "\n",
    "def format_financial_dataset(dataset):\n",
    "    \"\"\"Format dataset for instruction tuning\"\"\"\n",
    "    \n",
    "    def format_prompt(example):\n",
    "        if example.get(\"input\", \"\").strip():\n",
    "            prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
    "        else:\n",
    "            prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
    "        return {\"text\": prompt}\n",
    "    \n",
    "    formatted_dataset = dataset.map(format_prompt)\n",
    "    return formatted_dataset\n",
    "\n",
    "# Load and format dataset\n",
    "financial_dataset = load_financial_datasets()\n",
    "formatted_dataset = format_financial_dataset(financial_dataset)\n",
    "print(f\"Formatted dataset size: {len(formatted_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Tokenizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     36\u001b[39m     model.config.pretraining_tp = \u001b[32m1\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m model, tokenizer = \u001b[43msetup_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel and tokenizer loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36msetup_model_and_tokenizer\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     23\u001b[39m tokenizer.padding_side = \u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Load model with quantization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBASE_MODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Disable caching for training\u001b[39;00m\n\u001b[32m     35\u001b[39m model.config.use_cache = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/ml/financial-analyst/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    562\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    567\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    568\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/ml/financial-analyst/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3202\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3199\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3202\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\n\u001b[32m   3204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3205\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   3206\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/ml/financial-analyst/venv/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda.is_available():\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     65\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m         )\n",
      "\u001b[31mRuntimeError\u001b[39m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "# Model configuration based on search results\n",
    "BASE_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # or \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "NEW_MODEL_NAME = \"Llama-3-8B-Financial-LoRA\"\n",
    "\n",
    "def setup_model_and_tokenizer():\n",
    "    \"\"\"Setup quantized model and tokenizer with financial optimization\"\"\"\n",
    "    \n",
    "    # Quantization configuration for efficient training\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Disable caching for training\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = setup_model_and_tokenizer()\n",
    "print(\"Model and tokenizer loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lora_config():\n",
    "    \"\"\"Create LoRA configuration optimized for financial tasks\"\"\"\n",
    "    \n",
    "    # LoRA parameters based on search results and financial optimization\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=16,  # Low-rank dimension - balance between performance and efficiency\n",
    "        lora_alpha=16,  # Scaling factor (Î±/r ratio = 1.0)\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention layers\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
    "        ],\n",
    "        lora_dropout=0.1,  # Dropout for regularization\n",
    "        bias=\"none\",\n",
    "        use_rslora=False,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    \n",
    "    return lora_config\n",
    "\n",
    "# Apply LoRA to model\n",
    "lora_config = create_lora_config()\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    print(f\"Trainable params: {trainable_params:,}\")\n",
    "    print(f\"All params: {all_param:,}\")\n",
    "    print(f\"Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
    "\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_arguments():\n",
    "    \"\"\"Create training arguments optimized for financial fine-tuning\"\"\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/{NEW_MODEL_NAME}\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        warmup_steps=100,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        group_by_length=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=neptune,  # Can be set to \"neptune\" for experiment tracking\n",
    "    )\n",
    "    \n",
    "    return training_args\n",
    "\n",
    "training_args = create_training_arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune AI Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_neptune_tracking():\n",
    "    \"\"\"Setup Neptune AI for experiment tracking\"\"\"\n",
    "    \n",
    "    # Initialize Neptune (replace with your credentials)\n",
    "    run = neptune.init_run(\n",
    "        project=\"your-workspace/financial-llm\",\n",
    "        api_token=\"your-neptune-token\",\n",
    "        name=\"llama3-financial-lora\",\n",
    "        tags=[\"llama3\", \"lora\", \"finance\", \"fine-tuning\"]\n",
    "    )\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    run[\"parameters\"] = {\n",
    "        \"model_name\": BASE_MODEL_NAME,\n",
    "        \"lora_r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "    }\n",
    "    \n",
    "    return run\n",
    "\n",
    "# Uncomment to enable Neptune tracking\n",
    "# neptune_run = setup_neptune_tracking()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model, tokenizer, dataset, training_args):\n",
    "    \"\"\"Create SFT trainer for financial fine-tuning\"\"\"\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=dataset.select(range(min(100, len(dataset)))),  # Small eval set\n",
    "        peft_config=lora_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        max_seq_length=2048,\n",
    "        packing=True,  # Pack multiple samples into single sequence\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False,\n",
    "            \"append_concat_token\": False,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Create trainer\n",
    "trainer = create_trainer(model, tokenizer, formatted_dataset, training_args)\n",
    "\n",
    "# Start training\n",
    "print(\"Starting financial fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model()\n",
    "print(f\"Model saved to ./results/{NEW_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_financial_model(model, tokenizer):\n",
    "    \"\"\"Test the fine-tuned model on financial queries\"\"\"\n",
    "    \n",
    "    # Test prompts\n",
    "    test_prompts = [\n",
    "        \"### Instruction:\\nExplain what investors should look for in a company's 10-K filing.\\n\\n### Response:\\n\",\n",
    "        \"### Instruction:\\nWhat are the key financial ratios for evaluating a stock?\\n\\n### Response:\\n\",\n",
    "        \"### Instruction:\\nHow do interest rate changes affect stock market valuations?\\n\\n### Response:\\n\"\n",
    "    ]\n",
    "    \n",
    "    # Create pipeline for inference\n",
    "    pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=512,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(\"Testing fine-tuned financial model:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        print(\"Input:\", prompt.split(\"### Response:\")[0].replace(\"### Instruction:\\n\", \"\").strip())\n",
    "        \n",
    "        result = pipe(prompt)\n",
    "        response = result[0]['generated_text'].split(\"### Response:\\n\")[-1].strip()\n",
    "        \n",
    "        print(\"Output:\", response[:200] + \"...\" if len(response) > 200 else response)\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Test the model\n",
    "test_financial_model(model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_load_model():\n",
    "    \"\"\"Save and demonstrate loading the fine-tuned model\"\"\"\n",
    "    \n",
    "    # Save the model and tokenizer\n",
    "    model.save_pretrained(f\"./saved_models/{NEW_MODEL_NAME}\")\n",
    "    tokenizer.save_pretrained(f\"./saved_models/{NEW_MODEL_NAME}\")\n",
    "    \n",
    "    print(f\"Model and tokenizer saved to ./saved_models/{NEW_MODEL_NAME}\")\n",
    "    \n",
    "    # Demonstrate loading\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Load base model\n",
    "    base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        ),\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Load LoRA weights\n",
    "    financial_model = PeftModel.from_pretrained(\n",
    "        base_model_reload, \n",
    "        f\"./saved_models/{NEW_MODEL_NAME}\"\n",
    "    )\n",
    "    \n",
    "    print(\"Model successfully reloaded with LoRA weights\")\n",
    "    return financial_model\n",
    "\n",
    "# Save the model\n",
    "final_model = save_and_load_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
